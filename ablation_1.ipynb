{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0125 0.025  0.05   0.1    0.2    0.4    0.8   ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rates = 2**np.arange(7)/80\n",
    "print(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(sm):\n",
    "    seq_len = 220\n",
    "    sm = sm.split()\n",
    "    if len(sm)>218:\n",
    "        # print('SMILES is too long ({:d})'.format(len(sm)))\n",
    "        sm = sm[:109]+sm[-109:]\n",
    "    ids = [vocab.stoi.get(token, unk_index) for token in sm]\n",
    "    ids = [sos_index] + ids + [eos_index]\n",
    "    seg = [1]*len(ids)\n",
    "    padding = [pad_index]*(seq_len - len(ids))\n",
    "    ids.extend(padding), seg.extend(padding)\n",
    "    return ids, seg\n",
    "\n",
    "def get_array(smiles):\n",
    "    x_id, x_seg = [], []\n",
    "    for sm in smiles:\n",
    "        a,b = get_inputs(sm)\n",
    "        x_id.append(a)\n",
    "        x_seg.append(b)\n",
    "    return torch.tensor(x_id), torch.tensor(x_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4245037\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pretrain_trfm import TrfmSeq2seq\n",
    "from build_vocab import WordVocab\n",
    "from utils import split\n",
    "import torch.nn as nn\n",
    "\n",
    "pad_index = 0\n",
    "unk_index = 1\n",
    "eos_index = 2\n",
    "sos_index = 3\n",
    "mask_index = 4\n",
    "\n",
    "vocab = WordVocab.load_vocab('../models/vocab.pkl')\n",
    "\n",
    "trfm = TrfmSeq2seq(len(vocab), 256, len(vocab), 4)\n",
    "# trfm = nn.DataParallel(trfm)\n",
    "trfm.load_state_dict(torch.load('../models/trfm_12_23000.pkl'))\n",
    "trfm.eval()\n",
    "\n",
    "print('Total parameters:', sum(p.numel() for p in trfm.parameters()))\n",
    "# print(trfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST, RNN, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7334, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>class</th>\n",
       "      <th>synergy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C[C@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1O=c1[nH]c...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.693530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1O=c1[nH]...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.693530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C[C@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1C=CCn1c(=...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.248808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C[C@@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1C=CCn1c(...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.248808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C[C@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1CC(C)C[C@...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.237948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  class    synergy\n",
       "0  C[C@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1O=c1[nH]c...      0   7.693530\n",
       "1  C[C@@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1O=c1[nH]...      0   7.693530\n",
       "2  C[C@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1C=CCn1c(=...      0  10.248808\n",
       "3  C[C@@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1C=CCn1c(...      0  10.248808\n",
       "4  C[C@]1(c2nc3c(C(N)=O)cccc3[nH]2)CCCN1CC(C)C[C@...      0   7.237948"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data_drug/drug_ab_smiles/drug2058_abba_seq_class&synergy.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df,random_state=10,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>synergy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5867.000000</td>\n",
       "      <td>5867.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.103801</td>\n",
       "      <td>6.289857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305028</td>\n",
       "      <td>21.148259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-78.528867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.967280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.774369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.187697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>135.288435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class      synergy\n",
       "count  5867.000000  5867.000000\n",
       "mean      0.103801     6.289857\n",
       "std       0.305028    21.148259\n",
       "min       0.000000   -78.528867\n",
       "25%       0.000000    -5.967280\n",
       "50%       0.000000     2.774369\n",
       "75%       0.000000    15.187697\n",
       "max       1.000000   135.288435"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>class</th>\n",
       "      <th>synergy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>COC1=C2C[C@@H](C)C[C@H](OC)[C@H](O)[C@H](C)C=C...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.998418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>COc1cccc2c1C(=O)c1c(O)c3c(c(O)c1C2=O)C[C@@](O)...</td>\n",
       "      <td>0</td>\n",
       "      <td>12.392604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>Nc1ccn([C@H]2O[C@H](CO)[C@H](O)C2(F)F)c(=O)n1O...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.201511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>COC1=C2C[C@@H](C)C[C@H](OC)[C@H](O)[C@H](C)C=C...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.967876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5780</th>\n",
       "      <td>CCC1=C[C@H]2C[N@@](C1)Cc1c([nH]c3ccccc13)[C@](...</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.395957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles  class    synergy\n",
       "1090  COC1=C2C[C@@H](C)C[C@H](OC)[C@H](O)[C@H](C)C=C...      0   8.998418\n",
       "6044  COc1cccc2c1C(=O)c1c(O)c3c(c(O)c1C2=O)C[C@@](O)...      0  12.392604\n",
       "6066  Nc1ccn([C@H]2O[C@H](CO)[C@H](O)C2(F)F)c(=O)n1O...      0   8.201511\n",
       "4398  COC1=C2C[C@@H](C)C[C@H](OC)[C@H](O)[C@H](C)C=C...      0   5.967876\n",
       "5780  CCC1=C[C@H]2C[N@@](C1)Cc1c([nH]c3ccccc13)[C@](...      0  -3.395957"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['smiles']\n",
    "y_train = train['synergy']\n",
    "label_train = train['class']\n",
    "\n",
    "X_test = test['smiles']\n",
    "y_test = test['synergy']\n",
    "label_test = test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5867"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(x):\n",
    "    if x >= 30:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_acc(synergy,label):\n",
    "    acc = 0\n",
    "    sum = len(synergy)\n",
    "    for i in range(sum):\n",
    "        class_i = get_class(synergy[i])\n",
    "        if class_i == label[i]:\n",
    "            acc += 1\n",
    "    acc = acc/sum\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_regression(X, X_test, y, n_repeats):\n",
    "    \n",
    "    ret = []\n",
    "    for i in range(n_repeats):\n",
    "        reg = MLPRegressor(max_iter=1000, random_state=1)\n",
    "        reg.fit(X, y)\n",
    "    ret.append(reg.predict(X_test))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ablation(X, X_test, y, y_test, rate, n_repeats):\n",
    "    auc = np.empty(n_repeats)\n",
    "    precision=[]\n",
    "    for i in range(n_repeats):\n",
    "        clf = MLPClassifier(max_iter=1000)\n",
    "        if rate==1:\n",
    "            X_train, y_train = X,y\n",
    "        else:\n",
    "            X_train, _, y_train, __ = train_test_split(X, y, test_size=1-rate, stratify=y)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_score = clf.predict_proba(X_test)\n",
    "        auc[i] = roc_auc_score(y_test, y_score[:,1])\n",
    "    print('training set:',accuracy_score(y_train, clf.predict(X_train))) \n",
    "    print('testing set accuracy_score:',accuracy_score(y_test, clf.predict(X_test)))\n",
    "    print('testing set precision_score:',precision_score(y_test, clf.predict(X_test))) \n",
    "    print('testing set recall_score:',recall_score(y_test, clf.predict(X_test))) \n",
    "    print('testing set f1_score:',f1_score(y_test, clf.predict(X_test))) \n",
    "    ret = {}\n",
    "    ret['auc mean'] = np.mean(auc)\n",
    "    ret['auc std'] = np.std(auc)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5867 molecules. It will take a little time.\n",
      "(5867, 1024)\n",
      "There are 1467 molecules. It will take a little time.\n",
      "(1467, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_split = [split(sm) for sm in X_train.values]\n",
    "xid, _ = get_array(x_split)\n",
    "X = trfm.encode(torch.t(xid))\n",
    "print(X.shape)\n",
    "x_split = [split(sm) for sm in X_test.values]\n",
    "xid, _ = get_array(x_split)\n",
    "X_test = trfm.encode(torch.t(xid))\n",
    "print(X_test.shape)\n",
    "y, y_test = y_train.values, y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 4.066076 ,  6.7606597,  3.901361 , ..., -3.307425 , 22.167635 ,\n",
      "        2.0586295], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "score_dic = ablation_regression(X, X_test, y, 3)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8997955010224948"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_acc(score_dic[0],label_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9401235883230343\n",
      "testing set accuracy_score: 0.9270620313565099\n",
      "testing set precision_score: 0.7941176470588235\n",
      "testing set recall_score: 0.48502994011976047\n",
      "testing set f1_score: 0.6022304832713754\n",
      "{'auc mean': 0.9435929295255642, 'auc std': 0.0071875694385787085}\n"
     ]
    }
   ],
   "source": [
    "# .save_SmilesTransformer/trfm_B32_1_10000.pkl\n",
    "score_dic = ablation(X, X_test, label_train , label_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9396974216918815\n",
      "testing set accuracy_score: 0.9038854805725971\n",
      "testing set precision_score: 0.5855263157894737\n",
      "testing set recall_score: 0.5329341317365269\n",
      "testing set f1_score: 0.5579937304075235\n",
      "{'auc mean': 0.8645444495624137, 'auc std': 0.015441251002189491}\n"
     ]
    }
   ],
   "source": [
    "# tfevent/321/.save/trfm_B8_1_48.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9343703388024718\n",
      "testing set accuracy_score: 0.9045671438309475\n",
      "testing set precision_score: 0.6173913043478261\n",
      "testing set recall_score: 0.4251497005988024\n",
      "testing set f1_score: 0.5035460992907802\n",
      "{'auc mean': 0.8675191156149241, 'auc std': 0.013417317650156604}\n"
     ]
    }
   ],
   "source": [
    "# tfevent/321/.save/trfm_B8_1_16.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9322395056467079\n",
      "testing set accuracy_score: 0.8977505112474438\n",
      "testing set precision_score: 0.5562913907284768\n",
      "testing set recall_score: 0.5029940119760479\n",
      "testing set f1_score: 0.5283018867924528\n",
      "{'auc mean': 0.8740028788576693, 'auc std': 0.014486884712658812}\n"
     ]
    }
   ],
   "source": [
    "# models/vocab_AH_20W_shuffle.pkl .save/trfm_B8_1_69.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9132750905604091\n",
      "testing set accuracy_score: 0.8957055214723927\n",
      "testing set precision_score: 0.7058823529411765\n",
      "testing set recall_score: 0.1437125748502994\n",
      "testing set f1_score: 0.2388059701492537\n",
      "{'auc mean': 0.8637443574389682, 'auc std': 0.012722206945148282}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B32_1_17.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9226507564457703\n",
      "testing set accuracy_score: 0.9004771642808452\n",
      "testing set precision_score: 0.647887323943662\n",
      "testing set recall_score: 0.2754491017964072\n",
      "testing set f1_score: 0.38655462184873957\n",
      "{'auc mean': 0.8677441271303546, 'auc std': 0.016979602887987433}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B32_1_16.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9124227572981035\n",
      "testing set accuracy_score: 0.8936605316973415\n",
      "testing set precision_score: 0.5964912280701754\n",
      "testing set recall_score: 0.20359281437125748\n",
      "testing set f1_score: 0.3035714285714286\n",
      "{'auc mean': 0.858103523721787, 'auc std': 0.017492929747309004}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B32_1_10.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9456637545280204\n",
      "testing set accuracy_score: 0.9359236537150648\n",
      "testing set precision_score: 0.811965811965812\n",
      "testing set recall_score: 0.5688622754491018\n",
      "testing set f1_score: 0.6690140845070424\n",
      "{'auc mean': 0.9360002303086133, 'auc std': 0.01341711317644627}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B32_1_40000.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9586618367781803\n",
      "testing set accuracy_score: 0.9427402862985685\n",
      "testing set precision_score: 0.8029197080291971\n",
      "testing set recall_score: 0.6586826347305389\n",
      "testing set f1_score: 0.7236842105263158\n",
      "{'auc mean': 0.9526338093044681, 'auc std': 0.00818816387010075}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B128_1_2000.pkl smiles transformer\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9217984231834647\n",
      "testing set accuracy_score: 0.9038854805725971\n",
      "testing set precision_score: 0.7241379310344828\n",
      "testing set recall_score: 0.25149700598802394\n",
      "testing set f1_score: 0.37333333333333335\n",
      "{'auc mean': 0.8689531321971442, 'auc std': 0.014988574694371945}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B32_1_20000.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9970168335819305\n",
      "testing set accuracy_score: 0.9761417859577369\n",
      "testing set precision_score: 0.9230769230769231\n",
      "testing set recall_score: 0.8622754491017964\n",
      "testing set f1_score: 0.8916408668730652\n",
      "{'auc mean': 0.9940741593735606, 'auc std': 0.0012705126579486239}\n"
     ]
    }
   ],
   "source": [
    "# models/trfm_12_23000.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9343703388024718\n",
      "testing set accuracy_score: 0.9086571233810498\n",
      "testing set precision_score: 0.6633663366336634\n",
      "testing set recall_score: 0.40119760479041916\n",
      "testing set f1_score: 0.5\n",
      "{'auc mean': 0.8693302625518193, 'auc std': 0.01570973752850061}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B32_1_10000.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9249946729171106\n",
      "testing set accuracy_score: 0.9032038173142468\n",
      "testing set precision_score: 0.6373626373626373\n",
      "testing set recall_score: 0.3473053892215569\n",
      "testing set f1_score: 0.44961240310077516\n",
      "{'auc mean': 0.8684799631506218, 'auc std': 0.017962056956294988}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9303217558065203\n",
      "testing set accuracy_score: 0.9038854805725971\n",
      "testing set precision_score: 0.6625\n",
      "testing set recall_score: 0.31736526946107785\n",
      "testing set f1_score: 0.42914979757085014\n",
      "{'auc mean': 0.8661416397973285, 'auc std': 0.014636803320067903}\n"
     ]
    }
   ],
   "source": [
    "# .save/trfm_B32_1_40000.pkl\n",
    "score_dic = ablation(X, X_test, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 1.0\n",
      "testing set accuracy_score: 0.8207225630538514\n",
      "testing set precision_score: 0.24705882352941178\n",
      "testing set recall_score: 0.23728813559322035\n",
      "testing set f1_score: 0.2420749279538905\n",
      "0.0125 {'auc mean': 0.6130743879472694, 'auc std': 0.06823235555099305}\n",
      "training set: 0.9931506849315068\n",
      "testing set accuracy_score: 0.8302658486707567\n",
      "testing set precision_score: 0.1896551724137931\n",
      "testing set recall_score: 0.12429378531073447\n",
      "testing set f1_score: 0.1501706484641638\n",
      "0.025 {'auc mean': 0.6400324092322516, 'auc std': 0.04917043689826123}\n",
      "training set: 0.931740614334471\n",
      "testing set accuracy_score: 0.8466257668711656\n",
      "testing set precision_score: 0.25\n",
      "testing set recall_score: 0.13559322033898305\n",
      "testing set f1_score: 0.17582417582417584\n",
      "0.05 {'auc mean': 0.7031605570884247, 'auc std': 0.024656231437098563}\n",
      "training set: 0.8993174061433447\n",
      "testing set accuracy_score: 0.8575323790047716\n",
      "testing set precision_score: 0.3644067796610169\n",
      "testing set recall_score: 0.24293785310734464\n",
      "testing set f1_score: 0.2915254237288135\n",
      "0.1 {'auc mean': 0.7346526956597905, 'auc std': 0.01972222412206516}\n",
      "training set: 0.8772378516624041\n",
      "testing set accuracy_score: 0.8752556237218814\n",
      "testing set precision_score: 0.45714285714285713\n",
      "testing set recall_score: 0.1807909604519774\n",
      "testing set f1_score: 0.25910931174089064\n",
      "0.2 {'auc mean': 0.7638488153111725, 'auc std': 0.015456836981878256}\n",
      "training set: 0.8815004262574595\n",
      "testing set accuracy_score: 0.8650306748466258\n",
      "testing set precision_score: 0.40707964601769914\n",
      "testing set recall_score: 0.2598870056497175\n",
      "testing set f1_score: 0.31724137931034485\n",
      "0.4 {'auc mean': 0.7984209258529321, 'auc std': 0.013541006793863334}\n",
      "training set: 0.8819518431706798\n",
      "testing set accuracy_score: 0.8820722563053851\n",
      "testing set precision_score: 0.5227272727272727\n",
      "testing set recall_score: 0.2598870056497175\n",
      "testing set f1_score: 0.3471698113207547\n",
      "0.8 {'auc mean': 0.8320238251653309, 'auc std': 0.01803165357494491}\n",
      "0.7264590880367389\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler_X = preprocessing.StandardScaler().fit(X)\n",
    "scaler_X_test = preprocessing.StandardScaler().fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nrml = scaler_X.transform(X)\n",
    "X_test_nrml = scaler_X_test.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 0.9784785851267845\n",
      "testing set accuracy_score: 0.8922972051806408\n",
      "testing set precision_score: 0.5568862275449101\n",
      "testing set recall_score: 0.5254237288135594\n",
      "testing set f1_score: 0.5406976744186046\n",
      "{'auc mean': 0.888143914509701, 'auc std': 0.006005828892987175}\n"
     ]
    }
   ],
   "source": [
    "score_dic = ablation(X_nrml, X_test_nrml, y, y_test, 0.8, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.DataFrame(columns=['auc mean','auc std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36991/2650599633.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tt.append(score_dic,ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc mean</th>\n",
       "      <th>auc std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993295</td>\n",
       "      <td>0.001289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   auc mean   auc std\n",
       "0  0.993295  0.001289"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.append(score_dic,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc mean': 0.9932947926247099, 'auc std': 0.0012886099623777107}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set accuracy_score 0.8486707566462167\n",
      "testing set precision_score 0.35294117647058826\n",
      "testing set recall_score 0.3050847457627119\n",
      "testing set f1_score 0.3272727272727273\n",
      "0.0125 {'auc mean': 0.7755449349625543, 'auc std': 0.03249938289497124}\n",
      "training set 1.0\n",
      "testing set accuracy_score 0.8650306748466258\n",
      "testing set precision_score 0.4186046511627907\n",
      "testing set recall_score 0.3050847457627119\n",
      "testing set f1_score 0.3529411764705882\n",
      "0.025 {'auc mean': 0.808355450444532, 'auc std': 0.023537921110122945}\n",
      "training set 1.0\n",
      "testing set accuracy_score 0.89093387866394\n",
      "testing set precision_score 0.5521472392638037\n",
      "testing set recall_score 0.5084745762711864\n",
      "testing set f1_score 0.5294117647058824\n",
      "0.05 {'auc mean': 0.8571159068015592, 'auc std': 0.01586337916341544}\n",
      "training set 1.0\n",
      "testing set accuracy_score 0.9120654396728016\n",
      "testing set precision_score 0.6538461538461539\n",
      "testing set recall_score 0.576271186440678\n",
      "testing set f1_score 0.6126126126126126\n",
      "0.1 {'auc mean': 0.8978085446502868, 'auc std': 0.014693727661880979}\n",
      "training set 0.9948849104859335\n",
      "testing set accuracy_score 0.9325153374233128\n",
      "testing set precision_score 0.7349397590361446\n",
      "testing set recall_score 0.6892655367231638\n",
      "testing set f1_score 0.7113702623906706\n",
      "0.2 {'auc mean': 0.9489206411772436, 'auc std': 0.010825614076296428}\n",
      "training set 0.9974424552429667\n",
      "testing set accuracy_score 0.9550102249488752\n",
      "testing set precision_score 0.8284023668639053\n",
      "testing set recall_score 0.7909604519774012\n",
      "testing set f1_score 0.8092485549132948\n",
      "0.4 {'auc mean': 0.9802023387202734, 'auc std': 0.006644773155405981}\n",
      "training set 0.9931813339015555\n",
      "testing set accuracy_score 0.972733469665985\n",
      "testing set precision_score 0.9101796407185628\n",
      "testing set recall_score 0.8587570621468926\n",
      "testing set f1_score 0.8837209302325582\n",
      "0.8 {'auc mean': 0.9934211448342312, 'auc std': 0.0010809607867588496}\n",
      "0.8944812802272402\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.8616223585548739\n",
      "0.0125 {'auc mean': 0.7704288748740857, 'auc std': 0.03159579655650447}\n",
      "training set 1.0\n",
      "testing set 0.8827539195637355\n",
      "0.025 {'auc mean': 0.8075089782332588, 'auc std': 0.03201121647506722}\n",
      "training set 1.0\n",
      "testing set 0.89093387866394\n",
      "0.05 {'auc mean': 0.8610167301712434, 'auc std': 0.016249241983318504}\n",
      "training set 1.0\n",
      "testing set 0.9216087252897068\n",
      "0.1 {'auc mean': 0.902348026978496, 'auc std': 0.014260628865419078}\n",
      "training set 0.9982949701619779\n",
      "testing set 0.9318336741649625\n",
      "0.2 {'auc mean': 0.9483274208382604, 'auc std': 0.009964148407719748}\n",
      "training set 0.9880647911338448\n",
      "testing set 0.9556918882072256\n",
      "0.4 {'auc mean': 0.9780617965225771, 'auc std': 0.004422989085448162}\n",
      "training set 0.9889196675900277\n",
      "testing set 0.9686434901158828\n",
      "0.8 {'auc mean': 0.9928929619410504, 'auc std': 0.0016434623811392099}\n",
      "0.8943692556512817\n"
     ]
    }
   ],
   "source": [
    "# 原模型 A375\n",
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.8595773687798227\n",
      "0.0125 {'auc mean': 0.6810278673422386, 'auc std': 0.05991598929775121}\n",
      "training set 1.0\n",
      "testing set 0.8895705521472392\n",
      "0.025 {'auc mean': 0.76884914785813, 'auc std': 0.04369759902830329}\n",
      "training set 1.0\n",
      "testing set 0.8916155419222904\n",
      "0.05 {'auc mean': 0.8473691847075081, 'auc std': 0.026869982003979064}\n",
      "training set 1.0\n",
      "testing set 0.9202453987730062\n",
      "0.1 {'auc mean': 0.9059143251957623, 'auc std': 0.017334607558005605}\n",
      "training set 0.9974424552429667\n",
      "testing set 0.94546693933197\n",
      "0.2 {'auc mean': 0.9519284891754951, 'auc std': 0.011179505116320737}\n",
      "training set 0.9970161977834612\n",
      "testing set 0.9577368779822768\n",
      "0.4 {'auc mean': 0.979619760479042, 'auc std': 0.008483217930743317}\n",
      "training set 0.9904112507990624\n",
      "testing set 0.9713701431492843\n",
      "0.8 {'auc mean': 0.9938203592814372, 'auc std': 0.0013645939084935119}\n",
      "0.8755041620056591\n"
     ]
    }
   ],
   "source": [
    "# 原模型\n",
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.7750511247443763\n",
      "0.0125 {'auc mean': 0.5423682634730539, 'auc std': 0.04164323620546127}\n",
      "training set 1.0\n",
      "testing set 0.8629856850715747\n",
      "0.025 {'auc mean': 0.5656102026715799, 'auc std': 0.0398809067032774}\n",
      "training set 0.9863481228668942\n",
      "testing set 0.8302658486707567\n",
      "0.05 {'auc mean': 0.6148438507600186, 'auc std': 0.03275663123439158}\n",
      "training set 0.9453924914675768\n",
      "testing set 0.8725289706884799\n",
      "0.1 {'auc mean': 0.6748076923076923, 'auc std': 0.02488327410437577}\n",
      "training set 0.9232736572890026\n",
      "testing set 0.8520790729379687\n",
      "0.2 {'auc mean': 0.726570935052971, 'auc std': 0.03247922900789843}\n",
      "training set 0.9156010230179028\n",
      "testing set 0.8970688479890934\n",
      "0.4 {'auc mean': 0.7893422385997236, 'auc std': 0.019110275491432353}\n",
      "training set 0.9256339228638397\n",
      "testing set 0.9025221540558964\n",
      "0.8 {'auc mean': 0.839883809304468, 'auc std': 0.013466126996794045}\n",
      "0.6790609988813582\n"
     ]
    }
   ],
   "source": [
    "#chem24_B8_n4_1_2000_good.pkl\n",
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.8471074380165289\n",
      "0.0125 {'auc mean': 0.5999144403701764, 'auc std': 0.09827209115839779}\n",
      "training set 1.0\n",
      "testing set 0.7975206611570248\n",
      "0.025 {'auc mean': 0.6756521739130436, 'auc std': 0.07010837747785428}\n",
      "training set 1.0\n",
      "testing set 0.8574380165289256\n",
      "0.05 {'auc mean': 0.7237558931377686, 'auc std': 0.057347923310083135}\n",
      "training set 1.0\n",
      "testing set 0.8429752066115702\n",
      "0.1 {'auc mean': 0.7737471625632967, 'auc std': 0.040755992715846685}\n",
      "training set 1.0\n",
      "testing set 0.8842975206611571\n",
      "0.2 {'auc mean': 0.8664536406495549, 'auc std': 0.03089002089360336}\n",
      "training set 1.0\n",
      "testing set 0.9049586776859504\n",
      "0.4 {'auc mean': 0.9191286886677142, 'auc std': 0.015071255174312955}\n",
      "training set 1.0\n",
      "testing set 0.9545454545454546\n",
      "0.8 {'auc mean': 0.9679465688842326, 'auc std': 0.006992332519298442}\n",
      "0.7895140811693981\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.7603305785123967\n",
      "0.0125 {'auc mean': 0.596327380952381, 'auc std': 0.062254179560351}\n",
      "training set 1.0\n",
      "testing set 0.8305785123966942\n",
      "0.025 {'auc mean': 0.6541190476190476, 'auc std': 0.060432028307442036}\n",
      "training set 1.0\n",
      "testing set 0.8305785123966942\n",
      "0.05 {'auc mean': 0.6937916666666667, 'auc std': 0.056954516938663886}\n",
      "training set 1.0\n",
      "testing set 0.8099173553719008\n",
      "0.1 {'auc mean': 0.7314940476190477, 'auc std': 0.043360020481242705}\n",
      "training set 1.0\n",
      "testing set 0.8140495867768595\n",
      "0.2 {'auc mean': 0.7832023809523809, 'auc std': 0.050128942298926164}\n",
      "training set 1.0\n",
      "testing set 0.8553719008264463\n",
      "0.4 {'auc mean': 0.8459702380952381, 'auc std': 0.02607836597741803}\n",
      "training set 0.9935233160621761\n",
      "testing set 0.8925619834710744\n",
      "0.8 {'auc mean': 0.8925119047619047, 'auc std': 0.015425691419910915}\n",
      "0.7424880952380952\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异构体 部分参数3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.8631406761177753\n",
      "0.0125 {'auc mean': 0.5931571968971963, 'auc std': 0.08567331753717906}\n",
      "training set 1.0\n",
      "testing set 0.8827699018538713\n",
      "0.025 {'auc mean': 0.6294569745226255, 'auc std': 0.0984025964808309}\n",
      "training set 1.0\n",
      "testing set 0.8696837513631407\n",
      "0.05 {'auc mean': 0.7016110846241383, 'auc std': 0.05604483478615813}\n",
      "training set 1.0\n",
      "testing set 0.9056706652126499\n",
      "0.1 {'auc mean': 0.781064753108546, 'auc std': 0.03217618999780552}\n",
      "training set 0.9972677595628415\n",
      "testing set 0.9143947655398037\n",
      "0.2 {'auc mean': 0.8651728013975841, 'auc std': 0.022514302198750884}\n",
      "training set 1.0\n",
      "testing set 0.9242093784078517\n",
      "0.4 {'auc mean': 0.9252048873684979, 'auc std': 0.01143792120080787}\n",
      "training set 0.990450204638472\n",
      "testing set 0.9449291166848419\n",
      "0.8 {'auc mean': 0.9587510831625214, 'auc std': 0.007382435489095433}\n",
      "0.7792026830115871\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc mean': 0.7293981481481482, 'auc std': 0.05145690962211546}\n"
     ]
    }
   ],
   "source": [
    "score_dic = ablation(X, X_test, y, y_test, 1, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异构体 smiles-transformer全部参数4m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.8010899182561307\n",
      "0.0125 {'auc mean': 0.6504859976027025, 'auc std': 0.08215775891964465}\n",
      "training set 1.0\n",
      "testing set 0.8841961852861036\n",
      "0.025 {'auc mean': 0.7025553012967201, 'auc std': 0.07886442289359047}\n",
      "training set 1.0\n",
      "testing set 0.885558583106267\n",
      "0.05 {'auc mean': 0.7911408957175547, 'auc std': 0.03790122943690139}\n",
      "training set 1.0\n",
      "testing set 0.9032697547683923\n",
      "0.1 {'auc mean': 0.8692622861501581, 'auc std': 0.022978743015923626}\n",
      "training set 1.0\n",
      "testing set 0.9264305177111717\n",
      "0.2 {'auc mean': 0.9253100141658492, 'auc std': 0.015916858979249087}\n",
      "training set 0.9982949701619779\n",
      "testing set 0.9591280653950953\n",
      "0.4 {'auc mean': 0.9666710253895608, 'auc std': 0.01082714158647372}\n",
      "training set 0.9970161977834612\n",
      "testing set 0.9604904632152589\n",
      "0.8 {'auc mean': 0.9863528386182848, 'auc std': 0.0030272075772981755}\n",
      "0.8416826227058329\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异构体 smiles-transformer全部参数4m 测试集为0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.8719346049046321\n",
      "0.0125 {'auc mean': 0.6203160074098288, 'auc std': 0.08289079448661878}\n",
      "training set 1.0\n",
      "testing set 0.9019073569482289\n",
      "0.025 {'auc mean': 0.7258058188950638, 'auc std': 0.040188726534540364}\n",
      "training set 1.0\n",
      "testing set 0.9073569482288828\n",
      "0.05 {'auc mean': 0.7947412008281572, 'auc std': 0.0450251006818532}\n",
      "training set 1.0\n",
      "testing set 0.94141689373297\n",
      "0.1 {'auc mean': 0.8860411899313501, 'auc std': 0.021766271394637198}\n",
      "training set 0.9982935153583617\n",
      "testing set 0.9305177111716622\n",
      "0.2 {'auc mean': 0.9271875340525224, 'auc std': 0.023167510152352182}\n",
      "training set 0.9982949701619779\n",
      "testing set 0.9618528610354223\n",
      "0.4 {'auc mean': 0.9734869783153537, 'auc std': 0.010263736169350246}\n",
      "training set 0.9957374254049446\n",
      "testing set 0.9795640326975477\n",
      "0.8 {'auc mean': 0.987396752751444, 'auc std': 0.003164978364988497}\n",
      "0.8449964974548171\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异构体 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 1.0\n",
      "testing set 0.8778625954198473\n",
      "0.0125 {'auc mean': 0.6214138185437422, 'auc std': 0.07820667264685029}\n",
      "training set 1.0\n",
      "testing set 0.9002181025081788\n",
      "0.025 {'auc mean': 0.6588695785323276, 'auc std': 0.058637561495599094}\n",
      "training set 1.0\n",
      "testing set 0.8969465648854962\n",
      "0.05 {'auc mean': 0.7330053975423956, 'auc std': 0.03601249116074987}\n",
      "training set 0.994535519125683\n",
      "testing set 0.9045801526717557\n",
      "0.1 {'auc mean': 0.799248224309473, 'auc std': 0.031706709155339755}\n",
      "training set 1.0\n",
      "testing set 0.9231188658669575\n",
      "0.2 {'auc mean': 0.8860741840175672, 'auc std': 0.02184860573397175}\n",
      "training set 0.9986357435197817\n",
      "testing set 0.9471101417666303\n",
      "0.4 {'auc mean': 0.9360521727631171, 'auc std': 0.009552579365934809}\n",
      "training set 0.9945429740791268\n",
      "testing set 0.9547437295528899\n",
      "0.8 {'auc mean': 0.9721313158380112, 'auc std': 0.005012680227976288}\n",
      "0.8009706702209476\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异构体 AH head=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 0.8636363636363636\n",
      "testing set 0.9056706652126499\n",
      "0.0125 {'auc mean': 0.5674804160736098, 'auc std': 0.11636057389968579}\n",
      "training set 0.8888888888888888\n",
      "testing set 0.9056706652126499\n",
      "0.025 {'auc mean': 0.5488762080089645, 'auc std': 0.11546255930519383}\n",
      "training set 0.8791208791208791\n",
      "testing set 0.9056706652126499\n",
      "0.05 {'auc mean': 0.6138194137524231, 'auc std': 0.08298961574917427}\n",
      "training set 0.8852459016393442\n",
      "testing set 0.9056706652126499\n",
      "0.1 {'auc mean': 0.56953885986922, 'auc std': 0.1198735833605656}\n",
      "training set 0.8825136612021858\n",
      "testing set 0.9056706652126499\n",
      "0.2 {'auc mean': 0.6029071386065222, 'auc std': 0.07201458172370492}\n",
      "training set 0.8826739427012278\n",
      "testing set 0.9056706652126499\n",
      "0.4 {'auc mean': 0.6328553208075085, 'auc std': 0.015426635245275197}\n",
      "training set 0.8826739427012278\n",
      "testing set 0.9056706652126499\n",
      "0.8 {'auc mean': 0.6389284260126047, 'auc std': 0.007510254754619374}\n",
      "0.5963436833044076\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异构体 AH head=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 0.8888888888888888\n",
      "testing set 0.9032697547683923\n",
      "0.0125 {'auc mean': 0.6275525770949113, 'auc std': 0.12732481488768096}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.025 {'auc mean': 0.6584755366677563, 'auc std': 0.09410084765237711}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.05 {'auc mean': 0.6841048272856053, 'auc std': 0.04789084127280928}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.1 {'auc mean': 0.7045417892557481, 'auc std': 0.023893679367947424}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.2 {'auc mean': 0.7280195052849515, 'auc std': 0.017112710256834526}\n",
      "training set 0.8908780903665814\n",
      "testing set 0.9059945504087193\n",
      "0.4 {'auc mean': 0.7275602048599761, 'auc std': 0.012585136170698497}\n",
      "training set 0.8913043478260869\n",
      "testing set 0.9059945504087193\n",
      "0.8 {'auc mean': 0.7344867603792089, 'auc std': 0.010473086497636687}\n",
      "0.6949630286897369\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 0.8888888888888888\n",
      "testing set 0.9059945504087193\n",
      "0.0125 {'auc mean': 0.5981627983000981, 'auc std': 0.14962874803720835}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.025 {'auc mean': 0.6668453743053285, 'auc std': 0.10645086500893927}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.05 {'auc mean': 0.6851340307289965, 'auc std': 0.04707600568483058}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.1 {'auc mean': 0.6987043696197014, 'auc std': 0.04907458781001199}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.2 {'auc mean': 0.7181677018633541, 'auc std': 0.016180911881005816}\n",
      "training set 0.8934356351236147\n",
      "testing set 0.9019073569482289\n",
      "0.4 {'auc mean': 0.7315713196033562, 'auc std': 0.01308843822499298}\n",
      "training set 0.8913043478260869\n",
      "testing set 0.9059945504087193\n",
      "0.8 {'auc mean': 0.7340171079873598, 'auc std': 0.0027190097083513395}\n",
      "0.6903718146297421\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chembl24 8h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 0.8888888888888888\n",
      "testing set 0.9059945504087193\n",
      "0.0125 {'auc mean': 0.5994241037376049, 'auc std': 0.17471073971719772}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.025 {'auc mean': 0.6325602048599761, 'auc std': 0.14982370685101815}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.05 {'auc mean': 0.6717157022992264, 'auc std': 0.09601324731267054}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.1 {'auc mean': 0.6809921542987903, 'auc std': 0.03693464900256645}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.2 {'auc mean': 0.6969314590824889, 'auc std': 0.03244931946207068}\n",
      "training set 0.8908780903665814\n",
      "testing set 0.9059945504087193\n",
      "0.4 {'auc mean': 0.7122202244742291, 'auc std': 0.00361210747398511}\n",
      "training set 0.8913043478260869\n",
      "testing set 0.9059945504087193\n",
      "0.8 {'auc mean': 0.7087365152010461, 'auc std': 0.005963940043626651}\n",
      "0.6717971948504803\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chembl24_half 8h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10131\\anaconda3\\envs\\rdenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\10131\\anaconda3\\envs\\rdenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 0.8888888888888888\n",
      "testing set 0.9059945504087193\n",
      "0.0125 {'auc mean': 0.5796262395118231, 'auc std': 0.13777587029711505}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.025 {'auc mean': 0.6408041843739783, 'auc std': 0.07976302504772169}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.05 {'auc mean': 0.6588520213577421, 'auc std': 0.0576116114572321}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.1 {'auc mean': 0.675530129672006, 'auc std': 0.0083822044851723}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.2 {'auc mean': 0.6775792742726382, 'auc std': 0.007198420859464451}\n",
      "training set 0.8908780903665814\n",
      "testing set 0.9059945504087193\n",
      "0.4 {'auc mean': 0.6767543859649122, 'auc std': 0.00574297085060615}\n",
      "training set 0.8913043478260869\n",
      "testing set 0.9059945504087193\n",
      "0.8 {'auc mean': 0.6761828484254113, 'auc std': 0.007470953952460843}\n",
      "0.6550470119397874\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chembl24_17W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 0.8888888888888888\n",
      "testing set 0.9059945504087193\n",
      "0.0125 {'auc mean': 0.5357148305546474, 'auc std': 0.0973578326274796}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.025 {'auc mean': 0.5414852348261959, 'auc std': 0.0821974967797185}\n",
      "training set 0.8904109589041096\n",
      "testing set 0.9059945504087193\n",
      "0.05 {'auc mean': 0.5252103083796447, 'auc std': 0.08779165288193447}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.1 {'auc mean': 0.5470229922632669, 'auc std': 0.06702537163572068}\n",
      "training set 0.8907849829351536\n",
      "testing set 0.9059945504087193\n",
      "0.2 {'auc mean': 0.5935322000653809, 'auc std': 0.08788353288565419}\n",
      "training set 0.8908780903665814\n",
      "testing set 0.9059945504087193\n",
      "0.4 {'auc mean': 0.6266170861937452, 'auc std': 0.04094289347768922}\n",
      "training set 0.8913043478260869\n",
      "testing set 0.9059945504087193\n",
      "0.8 {'auc mean': 0.6552718753405252, 'auc std': 0.02099996805626154}\n",
      "0.5749792182319152\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41082 41082\n",
      "45 45\n"
     ]
    }
   ],
   "source": [
    "x,X,y = extract_morgan(df_train['smiles'].values, df_train['HIV_active'].values)\n",
    "print(len(X), len(y))\n",
    "x,X_test,y_test = extract_morgan(df_test['smiles'].values, df_test['HIV_active'].values)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0125 {'auc mean': 0.5419753086419754, 'auc std': 0.04503846854450115}\n",
      "0.025 {'auc mean': 0.5503086419753087, 'auc std': 0.0564072236675212}\n",
      "0.05 {'auc mean': 0.5694444444444444, 'auc std': 0.08849201955412457}\n",
      "0.1 {'auc mean': 0.5827160493827162, 'auc std': 0.11926366834883169}\n",
      "0.2 {'auc mean': 0.6459876543209876, 'auc std': 0.13795329598523573}\n",
      "0.4 {'auc mean': 0.755246913580247, 'auc std': 0.08677780763643579}\n",
      "0.8 {'auc mean': 0.8388888888888889, 'auc std': 0.06304612034579853}\n",
      "0.640652557319224\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation_hiv(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc mean': 0.8560185185185183, 'auc std': 0.03140384702527043}\n"
     ]
    }
   ],
   "source": [
    "score_dic = ablation_hiv(X, X_test, y, y_test, 1, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41082 molecules. It will take a little time.\n",
      "(41082, 1024)\n",
      "SMILES is too long (220)\n",
      "SMILES is too long (274)\n",
      "SMILES is too long (247)\n",
      "SMILES is too long (226)\n",
      "SMILES is too long (244)\n",
      "SMILES is too long (243)\n",
      "SMILES is too long (253)\n",
      "SMILES is too long (266)\n",
      "SMILES is too long (346)\n",
      "SMILES is too long (232)\n",
      "SMILES is too long (242)\n",
      "SMILES is too long (247)\n",
      "SMILES is too long (240)\n",
      "SMILES is too long (370)\n",
      "SMILES is too long (224)\n",
      "SMILES is too long (283)\n",
      "SMILES is too long (265)\n",
      "SMILES is too long (240)\n",
      "SMILES is too long (219)\n",
      "SMILES is too long (246)\n",
      "SMILES is too long (243)\n",
      "SMILES is too long (284)\n",
      "SMILES is too long (270)\n",
      "SMILES is too long (232)\n",
      "SMILES is too long (260)\n",
      "SMILES is too long (284)\n",
      "SMILES is too long (284)\n",
      "SMILES is too long (439)\n",
      "SMILES is too long (491)\n",
      "SMILES is too long (439)\n",
      "SMILES is too long (296)\n",
      "SMILES is too long (341)\n",
      "SMILES is too long (285)\n",
      "SMILES is too long (327)\n",
      "SMILES is too long (341)\n",
      "SMILES is too long (400)\n",
      "SMILES is too long (263)\n",
      "SMILES is too long (238)\n",
      "SMILES is too long (383)\n",
      "SMILES is too long (360)\n",
      "SMILES is too long (233)\n",
      "SMILES is too long (365)\n",
      "SMILES is too long (265)\n",
      "SMILES is too long (240)\n",
      "SMILES is too long (223)\n",
      "(45, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_split = [split(sm) for sm in df_train['smiles'].values]\n",
    "xid, _ = get_array(x_split)\n",
    "X = rnn.encode(torch.t(xid))\n",
    "print(X.shape)\n",
    "x_split = [split(sm) for sm in df_test['smiles'].values]\n",
    "xid, _ = get_array(x_split)\n",
    "X_test = rnn.encode(torch.t(xid))\n",
    "print(X_test.shape)\n",
    "y, y_test = df_train['HIV_active'].values, df_test['HIV_active'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0125 {'auc mean': 0.6413580246913579, 'auc std': 0.11069279967556718}\n",
      "0.025 {'auc mean': 0.6655864197530865, 'auc std': 0.056627812535610146}\n",
      "0.05 {'auc mean': 0.644753086419753, 'auc std': 0.07297865449282823}\n",
      "0.1 {'auc mean': 0.6932098765432098, 'auc std': 0.06506711622922942}\n",
      "0.2 {'auc mean': 0.7206018518518518, 'auc std': 0.08554099486619125}\n",
      "0.4 {'auc mean': 0.6810185185185185, 'auc std': 0.06736796063488669}\n",
      "0.8 {'auc mean': 0.7066358024691357, 'auc std': 0.05058786972816301}\n",
      "0.6790233686067019\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation_hiv(X, X_test, y, y_test, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc mean': 0.6912037037037038, 'auc std': 0.09106494426547503}\n"
     ]
    }
   ],
   "source": [
    "score_dic = ablation_hiv(X, X_test, y, y_test, 1, 20)\n",
    "print(score_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from data/hiv.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 34.156 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 1 took 35.162 s\n",
      "Loading shard 3 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 2 took 36.376 s\n",
      "Loading shard 4 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 3 took 35.868 s\n",
      "Loading shard 5 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 4 took 36.968 s\n",
      "Loading shard 6 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 5 took 0.968 s\n",
      "TIMING: dataset construction took 220.752 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "featurizer = dc.feat.ConvMolFeaturizer()\n",
    "loader = dc.data.CSVLoader(\n",
    "      tasks=['HIV_active'],\n",
    "      smiles_field='smiles',\n",
    "      featurizer=featurizer)\n",
    "dataset = loader.featurize('data/hiv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 62.290 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 20.102 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.422 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.842 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.622 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/honda/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed_metrics: [0.7407407407407407]\n",
      "TIMING: dataset construction took 0.252 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.058 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.823 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.617283950617284]\n",
      "TIMING: dataset construction took 0.437 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.790 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.036 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.45987654320987653]\n",
      "TIMING: dataset construction took 0.602 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.796 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.979 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6604938271604939]\n",
      "TIMING: dataset construction took 0.290 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.907 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.147 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.46604938271604934]\n",
      "TIMING: dataset construction took 0.532 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.164 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.800 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5030864197530864]\n",
      "TIMING: dataset construction took 0.337 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.288 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.886 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5154320987654322]\n",
      "TIMING: dataset construction took 0.306 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.950 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.854 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6265432098765431]\n",
      "TIMING: dataset construction took 0.533 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.520 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.534 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.3055555555555556]\n",
      "TIMING: dataset construction took 0.601 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.703 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.371 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5555555555555556]\n",
      "TIMING: dataset construction took 0.441 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.295 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.891 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6018518518518519]\n",
      "TIMING: dataset construction took 0.445 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.260 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.869 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6111111111111112]\n",
      "TIMING: dataset construction took 0.539 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.187 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.024 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.712962962962963]\n",
      "TIMING: dataset construction took 0.536 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.699 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.579 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6296296296296295]\n",
      "TIMING: dataset construction took 0.392 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.745 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.518 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7469135802469136]\n",
      "TIMING: dataset construction took 0.384 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.727 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.747 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.49074074074074076]\n",
      "TIMING: dataset construction took 0.597 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.467 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 43.646 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5740740740740742]\n",
      "TIMING: dataset construction took 0.495 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.901 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.727 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6635802469135803]\n",
      "TIMING: dataset construction took 0.456 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.953 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.202 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6419753086419753]\n",
      "TIMING: dataset construction took 0.538 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.650 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.512 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6820987654320987]\n",
      "0.0125 {'auc std': 0.10627271985136169, 'auc mean': 0.5902777777777778}\n",
      "TIMING: dataset construction took 0.817 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.123 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.492 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.537037037037037]\n",
      "TIMING: dataset construction took 0.931 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.380 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.629 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7592592592592593]\n",
      "TIMING: dataset construction took 0.862 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.497 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.132 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6419753086419753]\n",
      "TIMING: dataset construction took 0.993 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.953 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.951 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5895061728395061]\n",
      "TIMING: dataset construction took 0.971 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.091 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.817 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5864197530864197]\n",
      "TIMING: dataset construction took 0.741 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.882 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.331 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6265432098765431]\n",
      "TIMING: dataset construction took 1.016 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.630 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.317 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.34567901234567905]\n",
      "TIMING: dataset construction took 0.745 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.707 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.733 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7469135802469136]\n",
      "TIMING: dataset construction took 0.907 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.187 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.906 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6820987654320988]\n",
      "TIMING: dataset construction took 0.941 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.474 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.327 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6388888888888888]\n",
      "TIMING: dataset construction took 0.785 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.318 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.092 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5648148148148148]\n",
      "TIMING: dataset construction took 0.945 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.557 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.966 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5]\n",
      "TIMING: dataset construction took 0.837 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.593 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.240 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.75]\n",
      "TIMING: dataset construction took 0.773 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.405 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.367 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed_metrics: [0.6450617283950617]\n",
      "TIMING: dataset construction took 0.845 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.368 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.981 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6604938271604939]\n",
      "TIMING: dataset construction took 0.731 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.865 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.573 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5370370370370371]\n",
      "TIMING: dataset construction took 1.214 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.154 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.725 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8024691358024691]\n",
      "TIMING: dataset construction took 1.098 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 42.479 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.651 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.4012345679012346]\n",
      "TIMING: dataset construction took 0.906 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.692 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.998 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6975308641975309]\n",
      "TIMING: dataset construction took 1.148 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.566 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.291 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6481481481481481]\n",
      "0.025 {'auc std': 0.11309475463246396, 'auc mean': 0.6180555555555556}\n",
      "TIMING: dataset construction took 2.152 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.281 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.958 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7314814814814814]\n",
      "TIMING: dataset construction took 1.966 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.926 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.322 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6141975308641976]\n",
      "TIMING: dataset construction took 2.282 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.290 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.231 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6388888888888888]\n",
      "TIMING: dataset construction took 2.071 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.090 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.790 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6265432098765433]\n",
      "TIMING: dataset construction took 1.879 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.623 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.166 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6018518518518519]\n",
      "TIMING: dataset construction took 1.810 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.015 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.162 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.4598765432098766]\n",
      "TIMING: dataset construction took 1.716 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.215 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.408 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6450617283950617]\n",
      "TIMING: dataset construction took 2.682 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.464 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.899 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7685185185185186]\n",
      "TIMING: dataset construction took 1.749 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.289 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.121 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5709876543209876]\n",
      "TIMING: dataset construction took 1.605 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.645 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.189 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7932098765432098]\n",
      "TIMING: dataset construction took 1.889 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.204 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.883 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.49382716049382713]\n",
      "TIMING: dataset construction took 2.072 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.882 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.758 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8734567901234568]\n",
      "TIMING: dataset construction took 2.077 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.074 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.767 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.4320987654320988]\n",
      "TIMING: dataset construction took 2.178 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.869 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.601 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5895061728395061]\n",
      "TIMING: dataset construction took 1.900 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.567 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.280 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5925925925925926]\n",
      "TIMING: dataset construction took 2.052 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.053 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.718 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6790123456790124]\n",
      "TIMING: dataset construction took 2.180 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.348 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.304 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7253086419753086]\n",
      "TIMING: dataset construction took 1.925 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.093 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.836 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5030864197530864]\n",
      "TIMING: dataset construction took 2.500 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.667 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.996 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7438271604938271]\n",
      "TIMING: dataset construction took 1.910 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.209 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 41.725 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5802469135802468]\n",
      "0.05 {'auc std': 0.11241635190615207, 'auc mean': 0.6331790123456791}\n",
      "TIMING: dataset construction took 4.121 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.856 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.729 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.787037037037037]\n",
      "TIMING: dataset construction took 3.913 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.810 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.844 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.4382716049382716]\n",
      "TIMING: dataset construction took 3.896 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.099 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.812 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6388888888888888]\n",
      "TIMING: dataset construction took 3.976 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.273 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.318 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5679012345679013]\n",
      "TIMING: dataset construction took 4.032 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.878 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.855 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6481481481481481]\n",
      "TIMING: dataset construction took 3.512 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.666 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.300 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6759259259259258]\n",
      "TIMING: dataset construction took 4.287 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.514 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 36.613 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5401234567901234]\n",
      "TIMING: dataset construction took 4.005 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.509 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.128 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6820987654320987]\n",
      "TIMING: dataset construction took 3.768 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.242 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.010 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.521604938271605]\n",
      "TIMING: dataset construction took 3.799 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.627 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.220 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7407407407407407]\n",
      "TIMING: dataset construction took 4.253 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.222 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.973 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6697530864197531]\n",
      "TIMING: dataset construction took 4.430 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.027 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.736 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5555555555555556]\n",
      "TIMING: dataset construction took 4.439 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.070 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 40.478 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7561728395061729]\n",
      "TIMING: dataset construction took 4.083 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.458 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.607 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5740740740740741]\n",
      "TIMING: dataset construction took 4.591 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.404 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 36.156 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6141975308641976]\n",
      "TIMING: dataset construction took 3.844 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.681 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.364 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6666666666666667]\n",
      "TIMING: dataset construction took 3.748 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.870 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.210 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7067901234567902]\n",
      "TIMING: dataset construction took 3.790 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.506 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.456 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7283950617283951]\n",
      "TIMING: dataset construction took 4.056 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 39.208 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.989 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5432098765432098]\n",
      "TIMING: dataset construction took 3.797 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 38.409 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 37.644 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6697530864197532]\n",
      "0.1 {'auc std': 0.08782626914134467, 'auc mean': 0.6362654320987655}\n",
      "TIMING: dataset construction took 7.296 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 34.899 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.440 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5864197530864197]\n",
      "TIMING: dataset construction took 8.158 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.130 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.725 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7222222222222222]\n",
      "TIMING: dataset construction took 8.316 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.097 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.981 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7037037037037037]\n",
      "TIMING: dataset construction took 8.043 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 34.308 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.295 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7561728395061729]\n",
      "TIMING: dataset construction took 7.571 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.626 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.896 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6265432098765432]\n",
      "TIMING: dataset construction took 7.990 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.674 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.379 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7314814814814814]\n",
      "TIMING: dataset construction took 8.020 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 34.315 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.068 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6604938271604939]\n",
      "TIMING: dataset construction took 7.624 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.518 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.438 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.595679012345679]\n",
      "TIMING: dataset construction took 7.808 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.099 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.065 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7685185185185185]\n",
      "TIMING: dataset construction took 8.530 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.012 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.966 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6049382716049383]\n",
      "TIMING: dataset construction took 8.342 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.796 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.794 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5061728395061729]\n",
      "TIMING: dataset construction took 7.532 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 35.316 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.710 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7191358024691359]\n",
      "TIMING: dataset construction took 7.579 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.517 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.447 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5833333333333333]\n",
      "TIMING: dataset construction took 8.118 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.283 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.078 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6481481481481481]\n",
      "TIMING: dataset construction took 8.895 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 34.012 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.388 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.4876543209876544]\n",
      "TIMING: dataset construction took 7.336 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 34.127 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.934 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6635802469135803]\n",
      "TIMING: dataset construction took 8.511 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.474 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.328 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7191358024691358]\n",
      "TIMING: dataset construction took 8.231 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.776 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.425 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5802469135802469]\n",
      "TIMING: dataset construction took 7.504 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 33.728 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 35.353 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.595679012345679]\n",
      "TIMING: dataset construction took 8.227 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 32.723 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 32.592 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.617283950617284]\n",
      "0.2 {'auc std': 0.07716913531857271, 'auc mean': 0.6438271604938273}\n",
      "TIMING: dataset construction took 16.732 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.272 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.716 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.4537037037037037]\n",
      "TIMING: dataset construction took 15.886 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.192 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.132 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5740740740740741]\n",
      "TIMING: dataset construction took 16.026 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.997 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.861 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7685185185185186]\n",
      "TIMING: dataset construction took 17.079 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.343 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.798 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7037037037037037]\n",
      "TIMING: dataset construction took 16.112 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.557 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.961 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7253086419753086]\n",
      "TIMING: dataset construction took 17.089 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.497 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.690 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8765432098765432]\n",
      "TIMING: dataset construction took 17.014 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 26.276 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.067 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6111111111111112]\n",
      "TIMING: dataset construction took 16.184 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 26.154 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.922 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7191358024691358]\n",
      "TIMING: dataset construction took 16.114 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.361 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.245 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8117283950617284]\n",
      "TIMING: dataset construction took 16.276 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 26.132 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.016 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7253086419753086]\n",
      "TIMING: dataset construction took 16.541 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.869 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.690 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.9351851851851852]\n",
      "TIMING: dataset construction took 16.858 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.700 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.368 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6234567901234568]\n",
      "TIMING: dataset construction took 16.624 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 26.882 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.771 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5555555555555556]\n",
      "TIMING: dataset construction took 16.941 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.041 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.677 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8333333333333333]\n",
      "TIMING: dataset construction took 15.567 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.649 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.470 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7067901234567902]\n",
      "TIMING: dataset construction took 16.927 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.901 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.932 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6697530864197532]\n",
      "TIMING: dataset construction took 16.716 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.639 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.207 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7067901234567902]\n",
      "TIMING: dataset construction took 15.955 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.410 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.280 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5277777777777777]\n",
      "TIMING: dataset construction took 16.322 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.079 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 25.522 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7623456790123457]\n",
      "TIMING: dataset construction took 16.631 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.829 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 24.520 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.824074074074074]\n",
      "0.4 {'auc std': 0.11906651778595918, 'auc mean': 0.7057098765432099}\n",
      "TIMING: dataset construction took 33.680 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.666 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.429 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8148148148148149]\n",
      "TIMING: dataset construction took 33.389 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.036 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.915 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7006172839506173]\n",
      "TIMING: dataset construction took 33.793 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.533 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.381 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.867283950617284]\n",
      "TIMING: dataset construction took 33.571 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.582 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.510 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7345679012345678]\n",
      "TIMING: dataset construction took 34.356 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.866 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.671 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7345679012345678]\n",
      "TIMING: dataset construction took 36.090 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.091 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.851 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5709876543209876]\n",
      "TIMING: dataset construction took 33.673 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.834 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.742 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6759259259259258]\n",
      "TIMING: dataset construction took 34.925 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.872 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.687 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7962962962962963]\n",
      "TIMING: dataset construction took 35.327 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.233 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.009 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8240740740740741]\n",
      "TIMING: dataset construction took 32.753 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.161 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.964 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7191358024691358]\n",
      "TIMING: dataset construction took 33.874 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.338 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.322 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8919753086419753]\n",
      "TIMING: dataset construction took 33.200 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.177 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.922 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6820987654320988]\n",
      "TIMING: dataset construction took 35.773 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 8.555 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.085 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5987654320987654]\n",
      "TIMING: dataset construction took 35.332 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.510 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.368 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7314814814814814]\n",
      "TIMING: dataset construction took 33.929 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.334 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.216 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6512345679012346]\n",
      "TIMING: dataset construction took 33.538 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.017 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.748 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7345679012345678]\n",
      "TIMING: dataset construction took 33.991 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.511 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.009 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.617283950617284]\n",
      "TIMING: dataset construction took 35.724 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.756 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.869 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8981481481481481]\n",
      "TIMING: dataset construction took 34.311 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.467 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.197 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6975308641975309]\n",
      "TIMING: dataset construction took 35.026 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.120 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.847 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8055555555555556]\n",
      "0.8 {'auc std': 0.0914221288276763, 'auc mean': 0.7373456790123458}\n",
      "0.652094356261023\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.select(np.where(np.array(list(map(len, df['smiles'])))<=218)[0])\n",
    "test_data = dataset.select(np.where(np.array(list(map(len, df['smiles'])))>218)[0])\n",
    "\n",
    "scores = []\n",
    "for rate in rates:\n",
    "    score_dic = ablation_hiv_dc(train_data, test_data, rate, 20)\n",
    "    print(rate, score_dic)\n",
    "    scores.append(score_dic['auc mean'])\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 42.882 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/honda/anaconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed_metrics: [0.7407407407407407]\n",
      "TIMING: dataset construction took 42.898 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7438271604938271]\n",
      "TIMING: dataset construction took 43.568 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5987654320987654]\n",
      "TIMING: dataset construction took 42.949 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.478395061728395]\n",
      "TIMING: dataset construction took 43.000 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8148148148148149]\n",
      "TIMING: dataset construction took 45.167 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7067901234567902]\n",
      "TIMING: dataset construction took 44.496 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.808641975308642]\n",
      "TIMING: dataset construction took 43.518 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7438271604938271]\n",
      "TIMING: dataset construction took 43.286 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8333333333333333]\n",
      "TIMING: dataset construction took 43.622 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7006172839506173]\n",
      "TIMING: dataset construction took 42.693 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.9012345679012346]\n",
      "TIMING: dataset construction took 42.338 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5987654320987654]\n",
      "TIMING: dataset construction took 43.477 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8580246913580247]\n",
      "TIMING: dataset construction took 45.311 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.595679012345679]\n",
      "TIMING: dataset construction took 44.817 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8148148148148148]\n",
      "TIMING: dataset construction took 43.116 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7561728395061729]\n",
      "TIMING: dataset construction took 43.251 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.8209876543209877]\n",
      "TIMING: dataset construction took 42.695 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.7530864197530864]\n",
      "TIMING: dataset construction took 43.351 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.5401234567901234]\n",
      "TIMING: dataset construction took 43.081 s\n",
      "Loading dataset from disk.\n",
      "computed_metrics: [0.6759259259259258]\n",
      "0.8 {'auc std': 0.11010464515045111, 'auc mean': 0.7242283950617284}\n"
     ]
    }
   ],
   "source": [
    "score_dic = ablation_hiv_dc(train_data, test_data, 1, 20)\n",
    "print(rate, score_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
